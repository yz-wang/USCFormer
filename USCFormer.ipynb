{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USCFormer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import torch.nn.functional\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from timm.models.layers import to_2tuple, trunc_normal_\n",
    "from torchsummary import summary\n",
    "\n",
    "# own files import\n",
    "from datasets.train_data_functions import TrainData\n",
    "from datasets.val_data_functions import ValData\n",
    "from models import seg_network\n",
    "from models.USCFormer import USCFormer\n",
    "from models.SwinTransformer import SwinTransformerLayer\n",
    "from loss import MSSSIM, CR\n",
    "from utils import to_psnr, print_log, validation, adjust_learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.switch_backend('agg')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "#train_data_dir = './data/cityscapes/train'\n",
    "#val_data_dir = './data/cityscapes/val'\n",
    "\n",
    "train_data_dir = '/home/yan/projects/xjm/TransDehaze/data/cityscapes/train'\n",
    "val_data_dir = '/home/yan/projects/xjm/TransDehaze/data/cityscapes/val'\n",
    "\n",
    "# --- Gpu device --- #\n",
    "device_ids = [Id for Id in range(torch.cuda.device_count())]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Hyper-parameters for training ---\n",
      "learning_rate: 0.0002\n",
      "crop_size: [256, 256]\n",
      "train_batch_size: 4\n",
      "val_batch_size: 1\n",
      "alpha_loss: 1\n",
      "beta_loss: 0.02\n",
      "gamma_loss: 0\n"
     ]
    }
   ],
   "source": [
    "# --- Parse hyper-parameters  --- #\n",
    "parser = argparse.ArgumentParser(description='Hyper-parameters for network')\n",
    "parser.add_argument('-learning_rate', help='Set the learning rate', default=2e-4, type=float)\n",
    "parser.add_argument('-crop_size', help='Set the crop_size', default=[256, 256], nargs='+', type=int)\n",
    "parser.add_argument('-train_batch_size', help='Set the training batch size', default=4, type=int)\n",
    "parser.add_argument('-epoch_start', help='Starting epoch number of the training', default=0, type=int)\n",
    "parser.add_argument('-alpha_loss', help='Set the lambda in loss function', default=1, type=float)\n",
    "parser.add_argument('-beta_loss', help='Set the lambda in loss function', default=0.02, type=float)\n",
    "parser.add_argument('-gamma_loss', help='Set the lambda in loss function', default=0, type=float)\n",
    "parser.add_argument('-val_batch_size', help='Set the validation/test batch size', default=1, type=int)\n",
    "parser.add_argument('-exp_name', help='directory for saving the networks of the experiment', type=str,default='checkpoints')\n",
    "parser.add_argument('-seed', help='set random seed', default=19, type=int)\n",
    "parser.add_argument('-num_epochs', help='number of epochs', default=200, type=int)\n",
    "parser.add_argument(\"--model\", type=str, default='deeplabv3plus_mobilenet',\n",
    "                    choices=['deeplabv3_resnet50', 'deeplabv3plus_resnet50',\n",
    "                             'deeplabv3_resnet101', 'deeplabv3plus_resnet101',\n",
    "                             'deeplabv3_mobilenet', 'deeplabv3plus_mobilenet'], help='model name')\n",
    "parser.add_argument(\"--separable_conv\", action='store_true', default=False,\n",
    "                        help=\"apply separable conv to decoder and aspp\")\n",
    "parser.add_argument(\"--output_stride\", type=int, default=16, choices=[8, 16])\n",
    "parser.add_argument(\"--num_classes\", type=int, default=19, help=\"num classer (default:19)\")  #cityscapes默认19个类别\n",
    "parser.add_argument(\"--ckpt\", type=str, default=\"./snapshots/best_deeplabv3plus_mobilenet_cityscapes_os16.pth\",\n",
    "                    help=\"restore from checkpoint\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "learning_rate = args.learning_rate\n",
    "crop_size = args.crop_size\n",
    "train_batch_size = args.train_batch_size\n",
    "epoch_start = args.epoch_start\n",
    "alpha_loss = args.alpha_loss\n",
    "beta_loss = args.beta_loss\n",
    "gamma_loss = args.gamma_loss\n",
    "val_batch_size = args.val_batch_size\n",
    "exp_name = args.exp_name\n",
    "num_epochs = args.num_epochs\n",
    "models = args.model\n",
    "separable_conv = args.separable_conv\n",
    "output_stride = args.output_stride\n",
    "num_classes = args.num_classes\n",
    "ckpt = args.ckpt\n",
    "\n",
    "print('--- Hyper-parameters for training ---')\n",
    "print('learning_rate: {}\\ncrop_size: {}\\ntrain_batch_size: {}\\nval_batch_size: {}\\nalpha_loss: {}\\nbeta_loss: {}\\ngamma_loss: {}'.format(learning_rate, crop_size, train_batch_size, val_batch_size, alpha_loss, beta_loss, gamma_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\t19\n"
     ]
    }
   ],
   "source": [
    "#set seed\n",
    "seed = args.seed\n",
    "if seed is not None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed) \n",
    "    print('Seed:\\t{}'.format(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load training data and validation/test data --- #\n",
    "lbl_train_data_loader = DataLoader(TrainData(crop_size, train_data_dir), batch_size=train_batch_size, shuffle=True, num_workers=8)\n",
    "val_data_loader = DataLoader(ValData(val_data_dir), batch_size=val_batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USCFormer Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ConvStream module to process local information of input image\n",
    "class ConvStream4x(nn.Module):\n",
    "    \"\"\"\n",
    "    Use ConvStream module to process local information of input image\n",
    "\n",
    "    Input:\n",
    "        - x: (B, 3, H, W)\n",
    "    Output:\n",
    "        - result: (B, 32, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 2, stride = 2, padding = 0, bias = False)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 2, stride = 2, padding = 0, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gelu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        result = self.bn3(self.conv3(x))\n",
    "        return result\n",
    "\n",
    "class ConvStream2x(nn.Module):\n",
    "    \"\"\"\n",
    "    Use ConvStream module to process local information of input image\n",
    "\n",
    "    Input:\n",
    "        - x: (B, 3, H, W)\n",
    "    Output:\n",
    "        - result: (B, 32, H, W)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 2, stride = 2, padding = 0, bias = False)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gelu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        result = self.bn3(self.conv3(x))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image to Patch Embedding\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]\n",
    "        self.num_patches = self.H * self.W\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n",
    "                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pdb.set_trace()\n",
    "        x = self.proj(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        return x, H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoder of Transformer stem \n",
    "class EncoderTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, in_chans=3, num_classes=1000, embed_dims=[96, 192, 384, 768],\n",
    "                 num_heads=[3, 6, 12, 24], depths=[2, 2, 6, 2], window_size = 7):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.depths = depths\n",
    "\n",
    "\n",
    "        # Conv stem definitions\n",
    "        self.Conv_stem1 = ConvStream4x(in_chans, embed_dims[0])\n",
    "        self.Conv_stem2 = ConvStream2x(embed_dims[0], embed_dims[1])\n",
    "        self.Conv_stem3 = ConvStream2x(embed_dims[1], embed_dims[2])\n",
    "        self.Conv_stem4 = ConvStream2x(embed_dims[2], embed_dims[3])\n",
    "\n",
    "\n",
    "        # patch embedding definitions\n",
    "        self.patch_embed1 = PatchEmbed(img_size=img_size, patch_size=7, stride=4, in_chans=in_chans,\n",
    "                                              embed_dim=embed_dims[0])\n",
    "        self.patch_embed2 = PatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0],\n",
    "                                              embed_dim=embed_dims[1])\n",
    "        self.patch_embed3 = PatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1],\n",
    "                                              embed_dim=embed_dims[2])\n",
    "        self.patch_embed4 = PatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=embed_dims[2],\n",
    "                                              embed_dim=embed_dims[3])\n",
    "\n",
    "        self.block1 = nn.Sequential(*[\n",
    "            SwinTransformerLayer(\n",
    "                dim=embed_dims[0], num_heads=num_heads[0], window_size=window_size, shift_size=0 if (i % 2 == 0) else window_size // 2)\n",
    "            for i in range(depths[0])])\n",
    "\n",
    "        self.block2 = nn.Sequential(*[\n",
    "            SwinTransformerLayer(\n",
    "                dim=embed_dims[1], num_heads=num_heads[1], window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2)\n",
    "            for i in range(depths[1])])\n",
    "\n",
    "        self.block3 = nn.Sequential(*[\n",
    "            SwinTransformerLayer(\n",
    "                dim=embed_dims[2], num_heads=num_heads[2], window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2)\n",
    "            for i in range(depths[2])])\n",
    "\n",
    "        self.block4 = nn.Sequential(*[\n",
    "            SwinTransformerLayer(\n",
    "                dim=embed_dims[3], num_heads=num_heads[3], window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2)\n",
    "            for i in range(depths[3])])\n",
    "        self.norm = nn.BatchNorm2d(embed_dims[-1])\n",
    "\n",
    "        self.Conv1 = nn.Conv2d(embed_dims[0], embed_dims[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.Conv2 = nn.Conv2d(embed_dims[1], embed_dims[1], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.Conv3 = nn.Conv2d(embed_dims[2], embed_dims[2], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.Conv4 = nn.Conv2d(embed_dims[3], embed_dims[3], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def reset_drop_path(self, drop_path_rate):\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(self.depths))]\n",
    "        cur = 0\n",
    "        for i in range(self.depths[0]):\n",
    "            self.block1[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "        cur += self.depths[0]\n",
    "        for i in range(self.depths[1]):\n",
    "            self.block2[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "        cur += self.depths[1]\n",
    "        for i in range(self.depths[2]):\n",
    "            self.block3[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "        cur += self.depths[2]\n",
    "        for i in range(self.depths[3]):\n",
    "            self.block4[i].drop_path.drop_prob = dpr[cur + i]\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 1\n",
    "        # Conv branch\n",
    "        conv1 = self.Conv_stem1(x)\n",
    "\n",
    "        # Transformer branch\n",
    "        # B,C,W,H\n",
    "        x1, H1, W1 = self.patch_embed1(x)\n",
    "        Tran1 = self.block1(x1)\n",
    "\n",
    "        #stage1_input:(1,3,512,1024), conv1:(1,64,128,256),Tran1:(1,64,128,256)\n",
    "        y1 = conv1 + Tran1\n",
    "        y1 = self.Conv1(y1)\n",
    "        outs.append(y1)\n",
    "\n",
    "        # stage 2\n",
    "        # Conv branch\n",
    "        conv2 = self.Conv_stem2(y1)\n",
    "        # Transformer branch\n",
    "        x2, H2, W2 = self.patch_embed2(y1)\n",
    "        Tran2 = self.block2(x2)\n",
    "\n",
    "        # stage2_input:(1,64,128,256), conv2:(1,128,64,128),Tran2:(1,128,64,128)\n",
    "        y2 = conv2 + Tran2\n",
    "        y2 = self.Conv2(y2)\n",
    "        outs.append(y2)\n",
    "\n",
    "        \n",
    "        # stage 3\n",
    "        # Conv branch\n",
    "        conv3 = self.Conv_stem3(y2)\n",
    "        # Transformer branch\n",
    "        x3, H3, W3 = self.patch_embed3(y2)\n",
    "        Tran3 = self.block3(x3)\n",
    "\n",
    "        # stage3_input:(1,128,64,128), conv3:(1,320,32,64),Tran3:(1,320,32,64)\n",
    "        y3 = conv3 + Tran3\n",
    "        y3 = self.Conv3(y3)\n",
    "        outs.append(y3)\n",
    "\n",
    "\n",
    "        # stage 4\n",
    "        # Conv branch\n",
    "        conv4 = self.Conv_stem4(y3)\n",
    "        # Transformer branch\n",
    "        x4, H4, W4 = self.patch_embed4(y3)\n",
    "        x4 = self.block4(x4)\n",
    "        Tran4 = self.norm(x4)\n",
    "\n",
    "        # stage4_input:(1,320,32,64), conv4:(1,512,16,32),Tran4:(1,512,16,32)\n",
    "        y4 = conv4 + Tran4\n",
    "        y4 = self.Conv4(y4)\n",
    "        outs.append(y4)\n",
    "\n",
    "        return outs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        return x\n",
    "    \n",
    "class Tenc(EncoderTransformer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Tenc, self).__init__(\n",
    "            embed_dims=[96, 192, 384, 768], num_heads=[3, 6, 12, 24], depths=[2, 2, 6, 2],window_size=7\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convprojection_base(nn.Module):\n",
    "    def __init__(self, path=None, **kwargs):\n",
    "        super(convprojection_base,self).__init__()\n",
    "\n",
    "        # self.convd32x = UpsampleConvLayer(512, 512, kernel_size=4, stride=2)\n",
    "        self.convd16x = UpsampleConvLayer(768, 384, kernel_size=4, stride=2)\n",
    "        self.dense_4 = nn.Sequential(ResidualBlock(384))\n",
    "        self.convd8x = UpsampleConvLayer(384, 192, kernel_size=4, stride=2)\n",
    "        self.dense_3 = nn.Sequential(ResidualBlock(192))\n",
    "        self.convd4x = UpsampleConvLayer(192, 96, kernel_size=4, stride=2)\n",
    "        self.dense_2 = nn.Sequential(ResidualBlock(96))\n",
    "        self.convd2x = UpsampleConvLayer(96, 24, kernel_size=4, stride=2)\n",
    "        self.dense_1 = nn.Sequential(ResidualBlock(24))\n",
    "        self.convd1x = UpsampleConvLayer(24, 12, kernel_size=4, stride=2)\n",
    "        self.conv_output = ConvLayer(12, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.active = nn.Tanh()        \n",
    "\n",
    "    def forward(self,x1):\n",
    "        res16x = self.convd16x(x1[3])\n",
    "\n",
    "        if x1[2].shape[3] != res16x.shape[3] and x1[2].shape[2] != res16x.shape[2]:\n",
    "            p2d = (0,-1,0,-1)\n",
    "            res16x = F.pad(res16x,p2d,\"constant\",0)\n",
    "        elif x1[2].shape[3] != res16x.shape[3] and x1[2].shape[2] == res16x.shape[2]:\n",
    "            p2d = (0,-1,0,0)\n",
    "            res16x = F.pad(res16x,p2d,\"constant\",0)\n",
    "        elif x1[2].shape[3] == res16x.shape[3] and x1[2].shape[2] != res16x.shape[2]:\n",
    "            p2d = (0,0,0,-1)\n",
    "            res16x = F.pad(res16x,p2d,\"constant\",0)\n",
    "\n",
    "        res8x = self.dense_4(res16x) + x1[2]\n",
    "\n",
    "        res8x = self.convd8x(res8x)\n",
    "        res4x = self.dense_3(res8x) + x1[1]\n",
    "        res4x = self.convd4x(res4x)\n",
    "        res2x = self.dense_2(res4x) + x1[0]\n",
    "        res2x = self.convd2x(res2x)\n",
    "        x = res2x\n",
    "        x = self.dense_1(x)\n",
    "        x = self.convd1x(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following is the network\n",
    "class USCFormer(nn.Module):\n",
    "\n",
    "    def __init__(self, output_nc=3, num_classes=19, seg_dim =32, **kwargs):\n",
    "        super(USCFormer, self).__init__()\n",
    "\n",
    "        self.Tenc = Tenc()\n",
    "        \n",
    "        self.convproj = convprojection_base()\n",
    "\n",
    "        #self.clean = ConvLayer(8, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.clean = ConvLayer(12, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        #self.active = nn.Tanh()\n",
    "\n",
    "        # semantic fused\n",
    "        self.seg_convs = nn.Sequential(\n",
    "            RDB(num_classes),\n",
    "            nn.Conv2d(num_classes, seg_dim, 1, bias=True)\n",
    "        )\n",
    "\n",
    "        self.coarse_convs = nn.Sequential(\n",
    "            Depthwise_separable_conv(output_nc, seg_dim)\n",
    "        )\n",
    "\n",
    "        self.combine_convs = nn.Sequential(\n",
    "            nn.Conv2d(seg_dim, seg_dim, 3, 1, 1, bias=True),\n",
    "            nn.Conv2d(seg_dim, seg_dim, 3, 1, 1, bias=True),\n",
    "            nn.Conv2d(seg_dim, output_nc, 3, 1, 1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "\n",
    "        #print(x.shape)\n",
    "        #print(seg.shape)\n",
    "        \n",
    "        x1 = self.Tenc(x)\n",
    "\n",
    "        x = self.convproj(x1)\n",
    "\n",
    "        x = self.clean(x)\n",
    "\n",
    "        # semantic fused\n",
    "        f = self.coarse_convs(x) + self.seg_convs(seg)\n",
    "        x1 = self.combine_convs(f)\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the network --- #\n",
    "net = USCFormer()\n",
    "\n",
    "\n",
    "# --- Define the seg network --- #\n",
    "model_map = {\n",
    "    'deeplabv3_resnet50': seg_network.deeplabv3_resnet50,\n",
    "    'deeplabv3plus_resnet50': seg_network.deeplabv3plus_resnet50,\n",
    "    'deeplabv3_resnet101': seg_network.deeplabv3_resnet101,\n",
    "    'deeplabv3plus_resnet101': seg_network.deeplabv3plus_resnet101,\n",
    "    'deeplabv3_mobilenet': seg_network.deeplabv3_mobilenet,\n",
    "    'deeplabv3plus_mobilenet': seg_network.deeplabv3plus_mobilenet\n",
    "}\n",
    "num_classes = 19\n",
    "model = model_map[models](num_classes=num_classes, output_stride=output_stride)\n",
    "if separable_conv and 'plus' in model:\n",
    "    model.convert_to_separable_conv(model.classifier)\n",
    "\n",
    "\n",
    "# --- Multi-GPU --- #\n",
    "net = net.to(device)\n",
    "net = nn.DataParallel(net, device_ids=device_ids)\n",
    "\n",
    "\n",
    "# --- Load the seg network weight --- #\n",
    "checkpoint = torch.load(ckpt)\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "# --- Multi-GPU --- #\n",
    "model = model.to(device)\n",
    "model = nn.DataParallel(model, device_ids=device_ids)\n",
    "\n",
    "#summary(net,input_size=[(3, 672, 1120),(19, 672, 1120)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build optimizer --- #\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Define the L1 loss function --- #\n",
    "l1_loss = nn.L1Loss()\n",
    "\n",
    "# --- Define the ms_ssim loss function --- #\n",
    "msssim_loss = MSSSIM()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foggycityscapes old_val_psnr: 13.20, old_val_ssim: 0.0624, old_val_ciede: 22.5058\n"
     ]
    }
   ],
   "source": [
    "# --- Previous PSNR and SSIM in testing --- #\n",
    "net.eval()\n",
    "\n",
    "'''# load the lastest model\n",
    "initial_epoch = findLastCheckpoint(save_dir='./{}'.format(exp_name))\n",
    "if initial_epoch > 0:\n",
    "    print('resuming by loading epoch %d' % initial_epoch)\n",
    "    net.load_state_dict(torch.load(os.path.join('./{}'.format(exp_name), 'net_epoch%d.pth' % initial_epoch)))\n",
    "'''\n",
    "\n",
    "old_val_psnr, old_val_ssim, old_val_ciede = validation(net, model, val_data_loader, device, exp_name)\n",
    "\n",
    "print('foggycityscapes old_val_psnr: {0:.2f}, old_val_ssim: {1:.4f}, old_val_ciede: {2:.4f}'.format(old_val_psnr, old_val_ssim, old_val_ciede))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate sets to 0.0002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yan/anaconda3/envs/xjm/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yan/anaconda3/envs/xjm/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iteration: 0\n",
      "Epoch: 1, Iteration: 100\n",
      "Epoch: 1, Iteration: 200\n",
      "Epoch: 1, Iteration: 300\n",
      "Epoch: 1, Iteration: 400\n",
      "Epoch: 1, Iteration: 500\n",
      "Epoch: 1, Iteration: 600\n",
      "Epoch: 1, Iteration: 700\n",
      "Epoch: 1, Iteration: 800\n",
      "Epoch: 1, Iteration: 900\n",
      "Epoch: 1, Iteration: 1000\n",
      "Epoch: 1, Iteration: 1100\n",
      "Epoch: 1, Iteration: 1200\n",
      "Epoch: 1, Iteration: 1300\n",
      "Epoch: 1, Iteration: 1400\n",
      "Epoch: 1, Iteration: 1500\n",
      "Epoch: 1, Iteration: 1600\n",
      "Epoch: 1, Iteration: 1700\n",
      "Epoch: 1, Iteration: 1800\n",
      "Epoch: 1, Iteration: 1900\n",
      "Epoch: 1, Iteration: 2000\n",
      "Epoch: 1, Iteration: 2100\n",
      "Epoch: 1, Iteration: 2200\n",
      "foggycityscapes\n",
      "(2166s) Epoch [1/200], Train_PSNR:22.52, Val_PSNR:26.52, Val_SSIM:0.9040, Val_CIEDE:6.2823\n",
      "model saved\n",
      "Learning rate sets to 0.0002.\n",
      "Epoch: 2, Iteration: 0\n",
      "Epoch: 2, Iteration: 100\n",
      "Epoch: 2, Iteration: 200\n",
      "Epoch: 2, Iteration: 300\n",
      "Epoch: 2, Iteration: 400\n",
      "Epoch: 2, Iteration: 500\n",
      "Epoch: 2, Iteration: 600\n",
      "Epoch: 2, Iteration: 700\n",
      "Epoch: 2, Iteration: 800\n",
      "Epoch: 2, Iteration: 900\n",
      "Epoch: 2, Iteration: 1000\n",
      "Epoch: 2, Iteration: 1100\n",
      "Epoch: 2, Iteration: 1200\n",
      "Epoch: 2, Iteration: 1300\n",
      "Epoch: 2, Iteration: 1400\n",
      "Epoch: 2, Iteration: 1500\n",
      "Epoch: 2, Iteration: 1600\n",
      "Epoch: 2, Iteration: 1700\n",
      "Epoch: 2, Iteration: 1800\n",
      "Epoch: 2, Iteration: 1900\n",
      "Epoch: 2, Iteration: 2000\n",
      "Epoch: 2, Iteration: 2100\n",
      "Epoch: 2, Iteration: 2200\n",
      "foggycityscapes\n",
      "(2212s) Epoch [2/200], Train_PSNR:24.91, Val_PSNR:25.30, Val_SSIM:0.9181, Val_CIEDE:5.5826\n",
      "Learning rate sets to 0.0002.\n",
      "Epoch: 3, Iteration: 0\n",
      "Epoch: 3, Iteration: 100\n",
      "Epoch: 3, Iteration: 200\n",
      "Epoch: 3, Iteration: 300\n",
      "Epoch: 3, Iteration: 400\n",
      "Epoch: 3, Iteration: 500\n",
      "Epoch: 3, Iteration: 600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m L1_loss \u001b[38;5;241m=\u001b[39m l1_loss(pred_image, gt)\n\u001b[1;32m     24\u001b[0m ms_ssim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmsssim_loss(pred_image, gt)\n\u001b[0;32m---> 26\u001b[0m cr_loss \u001b[38;5;241m=\u001b[39m \u001b[43mCR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mContrastLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m contrast_loss \u001b[38;5;241m=\u001b[39m cr_loss(pred_image, gt, input_image)\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m L1_loss  \u001b[38;5;241m+\u001b[39m alpha_loss \u001b[38;5;241m*\u001b[39m ms_ssim \u001b[38;5;241m+\u001b[39m beta_loss \u001b[38;5;241m*\u001b[39m contrast_loss\n",
      "File \u001b[0;32m~/projects/xjm/USCFormer-main/loss/CR.py:46\u001b[0m, in \u001b[0;36mContrastLoss.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28msuper\u001b[39m(ContrastLoss, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvgg \u001b[38;5;241m=\u001b[39m \u001b[43mVgg19\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss()\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]\n",
      "File \u001b[0;32m~/projects/xjm/USCFormer-main/loss/CR.py:14\u001b[0m, in \u001b[0;36mVgg19.__init__\u001b[0;34m(self, requires_grad)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Vgg19, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 14\u001b[0m     vgg_pretrained_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvgg19\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential()\n",
      "File \u001b[0;32m~/anaconda3/envs/xjm/lib/python3.8/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/xjm/lib/python3.8/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/xjm/lib/python3.8/site-packages/torchvision/models/vgg.py:467\u001b[0m, in \u001b[0;36mvgg19\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"VGG-19 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m weights \u001b[38;5;241m=\u001b[39m VGG19_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[0;32m--> 467\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vgg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/xjm/lib/python3.8/site-packages/torchvision/models/vgg.py:103\u001b[0m, in \u001b[0;36m_vgg\u001b[0;34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weights\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m         _ovewrite_named_param(kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(weights\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m--> 103\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mVGG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfgs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(weights\u001b[38;5;241m.\u001b[39mget_state_dict(progress\u001b[38;5;241m=\u001b[39mprogress))\n",
      "File \u001b[0;32m~/anaconda3/envs/xjm/lib/python3.8/site-packages/torchvision/models/vgg.py:44\u001b[0m, in \u001b[0;36mVGG.__init__\u001b[0;34m(self, features, num_classes, init_weights, dropout)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m features\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool2d((\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     45\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     46\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39mdropout),\n\u001b[1;32m     47\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m4096\u001b[39m, \u001b[38;5;241m4096\u001b[39m),\n\u001b[1;32m     48\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     49\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39mdropout),\n\u001b[1;32m     50\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m4096\u001b[39m, num_classes),\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init_weights:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules():\n",
      "File \u001b[0;32m~/anaconda3/envs/xjm/lib/python3.8/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/xjm/lib/python3.8/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/anaconda3/envs/xjm/lib/python3.8/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for epoch in range(initial_epoch, num_epochs):\n",
    "for epoch in range(epoch_start, num_epochs):\n",
    "    psnr_list = []\n",
    "    start_time = time.time()\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "    for batch_id, train_data in enumerate(lbl_train_data_loader):\n",
    "\n",
    "        input_image, gt = train_data\n",
    "        input_image = input_image.to(device)\n",
    "        gt = gt.to(device)\n",
    "\n",
    "        # --- Zero the parameter gradients --- #\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # --- Forward + Backward + Optimize --- #\n",
    "        net.train()\n",
    "        model.eval()\n",
    "\n",
    "        seg = model(input_image).to(device)\n",
    "        pred_image = net(input_image, seg)\n",
    "\n",
    "        L1_loss = l1_loss(pred_image, gt)\n",
    "        ms_ssim = -msssim_loss(pred_image, gt)\n",
    "\n",
    "        cr_loss = CR.ContrastLoss()\n",
    "        contrast_loss = cr_loss(pred_image, gt, input_image)\n",
    "\n",
    "        loss = L1_loss  + alpha_loss * ms_ssim + beta_loss * contrast_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- To calculate average PSNR --- #\n",
    "        psnr_list.extend(to_psnr(pred_image, gt))\n",
    "\n",
    "        if not (batch_id % 100):\n",
    "            print('Epoch: {0}, Iteration: {1}'.format(epoch+1, batch_id))\n",
    "\n",
    "    # --- Calculate the average training PSNR in one epoch --- #\n",
    "    train_psnr = sum(psnr_list) / len(psnr_list)\n",
    "\n",
    "    # --- Save the network parameters --- #\n",
    "    torch.save(net.state_dict(), './{}/latest'.format(exp_name))\n",
    "    #torch.save(net.state_dict(), './{}/net_epoch{}.pth'.format(exp_name, str(epoch + 1)))\n",
    "    \n",
    "    # --- Use the evaluation model in testing --- #\n",
    "    net.eval()\n",
    "\n",
    "    val_psnr, val_ssim, val_ciede = validation(net, model, val_data_loader, device, exp_name)\n",
    "\n",
    "    one_epoch_time = time.time() - start_time\n",
    "\n",
    "    print(\"foggycityscapes\")\n",
    "    print_log(epoch+1, num_epochs, one_epoch_time, train_psnr, val_psnr, val_ssim, val_ciede, exp_name)\n",
    "\n",
    "    # --- update the network weight --- #\n",
    "    if val_psnr >= old_val_psnr:\n",
    "        torch.save(net.state_dict(), './{}/best'.format(exp_name))\n",
    "        print('model saved')\n",
    "        old_val_psnr = val_psnr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
